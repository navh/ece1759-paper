\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}

\usepackage{tikz}
\usepackage{amsmath}
\usepackage{filecontents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Motivation: Parsers out in the wild are bad, not necessarily just CLP
% When profiled, during 'compression', CLP does spend ~50% of its time in 'parsing' loop
%
%Parsers out in the wild are bad, as others add
%
% ./tools/scripts/utils/run-in-container.sh cmake -S . -B multi-node-build
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%
% 1)yang- Figure out how to do a string compare with 'intrinsics'
% 2)yang- STRCMP, it may not be strcmp, if they're using something else we'll have to re-write that version.
% 3)yang - try to install.
% 4)amos- Find where they are currently parsing, and see if it can be pulled out/replaced easily.
% 5)amos- Be ready to test (be able to time just the parser/prove parser with some wait takes longer or something trivial)
% Thursday (try to text), Prefer friday night.
%%%%%%%%%%%
%-------------------------------------------------------------------------------
\begin{filecontents}{\jobname.bib}
%-------------------------------------------------------------------------------
@dataset{hadoop14tbdataset,
  author       = {Kirk Rodrigues and
                  Yu Luo and
                  Ding Yuan},
  title        = {hadoop-14TB-part1},
  month        = sep,
  year         = 2022,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.7114847},
  url          = {https://doi.org/10.5281/zenodo.7114847}
}
@inproceedings{rodrigues2021clp,
  title={{CLP}: Efficient and Scalable Search on Compressed Text Logs},
  author={Rodrigues, Kirk and Luo, Yu and Yuan, Ding},
  booktitle={15th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21)},
  pages={183--198},
  year={2021}
}
\end{filecontents}
%-------------------------------------------------------------------------------

\begin{document}

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf TODO:title Improved Parsing of Unstructured Logs using XYZ}

\author{
  {\rm Yang Su, Amos Hebb}\\
  University of Toronto
}

\maketitle

\begin{abstract}
  TODO:Abstract Your abstract text goes here. Just a few facts. Whet our appetites.
  Not more than 200 words, if possible, and preferably closer to 150.
\end{abstract}


\section{Introduction}

CLP~\cite{rodrigues2021clp} is a tool capable of compressing unstructured text logs. The CLP implementation of log ingestion TODO:FluffThisUp. This paper presents an implementatnion of an improved log ingestion process TODO:ProcessName . TODO:ProcessName achieves over TODO:Nx increase in ingestion performance. TODO:ProcessName does so without sacraficing compression ratio, or search performance. TODO:ProcessName's gains come from TODO:PapersGimmick.

\section{Design Overview}

TODO:DesignOverview

\section{Parsing Messages}

TODO:ParsingMessages

\subsection{Handling Ambiguous Tokens}
\subsection{Handling Special Cases}
%Not sure we include these, just quickly scimming CLP paper for headings we may care about
TODO:keepOrRemoveThese

\section{Evaluation}
%From CLP paper. --- CLP has been used to compress petabytes of logs from hundreds of different applications, and we have verified that its compression is lossless in all cases. Our evaluation focuses on CLP's performance. Specifically, we explore: 1) CLP's compression ration and speed; 2) CLP's search performance; and 3) CLP's scalability and resource efficiency.
TODO:Evaluation
We explore: 1) Change in compression speed; 2) Change in compression ratio; and 3) Change in resource efficiency

\subsection{Experiment Setup}
We used the Hadoop-14TB logs~\cite{hadoop14tbdataset} were generated by three Hadoop clusters, each containing 48 data nodes, running workloads from the HiBench Benchmark Suite [25] for a month. Note that the dataset generated by a benchmarking tool may be artificially uniform, as benchmarks do not always capture the randomness of real-world deployments. However, this should not affect our claims as we compare CLP with TODO:ProcessName relative to CLP's default log processing on the same dataset.

% amos- I think performing experiments in GCP, and renting huge ramdisks makes the most sense.
% amos- We could also ask dryuan for access to a node in his cluster, 
% amos- In the CLP paper the ingest speed is only a 30gb subset in a ramdisk.
% amos- In GCP we could do a similar 30gb in ramdisk on a 128gb ram instance
% amos- A cheap ol' e2-standard-32 has 32vcpu-128gb would be the easiest
% amos- CLP Paper uses an 8cpu-128gb, e2-highmem-16 might be closer
% amos- CLP Paper uses Haswell processors, everything GCP offers is newer.
% amos- For kicks (also because they're free until next year), I'd love to try a t2a machine
% amos- The craziest version of this would involve an entire 48cpu-192gb t2a.
% amos- A 30GB file at 500MB/s (from clp fig 7) would be ~1min. I'd be tempted to try bigger files.

\subsection{Compression Speed}

To examine the change in single-node ingestion speed, a 30GB subset of the Hadoop corpus was written to a tempfs RAM disk. The RAM Disk minimizes I/O overhead to fully expose algorithmic differences between the two versions of CLP. We use CLP's default compression level for both tests.

% -- TODO: I think maybe multithreading is the problem, on the bottom of p192 of clp paper they mention that they aren't all single-threaded in the para right below fig6, but then on the last para they mention that Elastic and Splunk are 'designed as multithreaded tools'. Is CLP the slow one here? If we just make this concurrent might we be done? 

\subsection{Compression Ratio}

TODO:CompressionRatio The new parser produces a nearly identical output, except for TODO:ABC. To verify that compression ratio was not impacted, the entire 14TB Hadoop corpus was compressed. With the improved parser, a compression ratio of TODO:CompressionRatioImproved was acheived, within TODO:NPercent of the default parser.

\subsection{Resource Efficiency}

CLP paper asserts "Compression and search are embarrassingly paralelizable". I don't think they actually demonstrate parallel compression, but do have a whole para about whacky IO. We'll need to work out how they measure this, or maybe we just drop the whole section. This feels pretty thin on benchmarking though and I get the feeling DrYuan is big on 'actually running stuff'.


%from CLP para2.4- "In order for CLP's compression and search to scale in a dsitributed system, each archive is independent of other archives and immutable once written. This independence makes compression easily parallelizable without any synchronization between threads writing different archives. The immutability ensures that a search thread can query an archive without synchronizing with a compression head. To avoid coordination between search threads, each archive is only queried by a single thread for a given query. Thus, CLP parallelizes compression and search at the granularity of individual archives.

% % amos- BEGIN_TIPS these are just tips from the template
% Footnotes should be places after punctuation characters, without any
% spaces between said characters and footnotes, like so.%
% \footnote{Remember that USENIX format stopped using endnotes and is
%   now using regular footnotes.} And some embedded literal code may
% look as follows.

% \begin{verbatim}
% int main(int argc, char *argv[]) 
% {
%     return 0;
% }
% \end{verbatim}
% % amos- END_TIPS


\section{incoherent ranting}

I have downloaded CLP.

There's a few spots where it reaches out looking for x86 specific binaries, seems like turning most of these off works.

I've managed to get it installed and running on a docker image using ubuntu:latest.

It runs, and when I compress about 2GB logs from my macbook's disk
\begin{enumerate}
  \item it takes about 18 seconds to do heuristic based, roughly 100MB/s, 5x slower than ramdisk in paper.
  \item I currently just get flaming segmentation faults whenever I try to use schemas.
\end{enumerate}

\section*{Acknowledgments}
TODO:Scrap this section? It's from the USENIX template. If we get access to the cluster or something may we could praise that, feels awkward to ack the existing paper though.
% The USENIX latex style is old and very tired, which is why
% there's no \textbackslash{}acks command for you to use when
% acknowledging. Sorry.

\section*{Availability}

HDFS Corpus is available at \cite{hadoop14tbdataset}. The fork of CLP is available at TODO:Repo.

% USENIX program committees give extra points to submissions that are
% backed by artifacts that are publicly available. If you made your code
% or data available, it's worth mentioning this fact in a dedicated
% section.

%-------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{\jobname}

\end{document}